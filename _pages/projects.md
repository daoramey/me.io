---
layout: single
title: Projects
---

Welcome to my projects page! Here, youâ€™ll find a collection of my work in **research and development**. Each project includes a brief description, key technologies used, and links to related resources.

---

### ğŸ•¶ï¸ **Social Interaction with Virtual Agent**
ğŸ“‚ **GitHub Repo**: View Code  
ğŸ“„ **Research Paper**: Read Here  
ğŸš€ **Description**:
Developed a full-body photorealistic agent avatar for social VR & human-agent interaction. The avatar includes facial expressions, eye gaze tracking, blinking, and body gestures to enhance interaction realism. Integrated into an interactive VR environment where the agent adapts based on user behavior.  
ğŸ›  **Technologies**:
Unity for VR environment and avatar rendering
Unreal Engine (MetaHuman) for photorealistic avatar creation
OpenFace & MediaPipe for real-time facial expression & eye gaze tracking
VRChat SDK / OpenXR for VR interaction
Machine Learning for behavior adaptation

---

### ğŸ”¬ **Interpersonal Coordination Analysis - CRQA**
ğŸ“‚ **GitHub Repo**: [View Code](https://github.com/daoramey/nonverbal-dynamics-VC-delay)  
ğŸš€ **Description**: Code for analyzing **interpersonal coordination of body movement and facial expression** using **Cross-Recurrence Quantification Analysis (CRQA)**.  
ğŸ›  **Technologies**: R, CRQA library

---

### ğŸ”¬ **Interpersonal Coordination Analysis - WLCC**
ğŸ“‚ **GitHub Repo**: [View Code](https://github.com/daoramey/nonverbal-dynamics-VC-resolution)  
ğŸš€ **Description**: Code for analyzing **interpersonal coordination of facial expression and motion tracking** using **Windowed Lagged Cross-Correlation (WLCC)**.  
ğŸ›  **Technologies**: R, rMEA library

---

### ğŸ­ **Non-Verbal Communication Cues Extraction**
ğŸ“‚ **GitHub Repo**: [View Code](https://github.com/daoramey/nonverbal-cues-extraction)  
ğŸš€ **Description**: A toolkit for extracting **facial expression, eye gaze, hands and body motion tracking** features from video recordings.  
ğŸ›  **Technologies**: Python, OpenFace, OpenPose, MediaPipe, OpenSmile, Optical Flow

---

### ğŸ­ **Speech Activity and Turn-taking Analysis**
ğŸ“‚ **GitHub Repo**: [View Code](https://github.com/your-repo)  
ğŸš€ **Description**: A toolkit for extracting **facial expression, eye gaze, and body motion tracking** features from audio recordings.  
ğŸ›  **Technologies**: Python

---

### ğŸ“œ **Deep Learning-Powered Movie Trailer Generation**
ğŸ“‚ **GitHub Repo**: [View Code](https://github.com/daoramey/movie-to-trailer) 
ğŸš€ **Description**: This project focuses on automating movie trailer generation using CNN and LSTM models. The system analyzes movie scenes and selects the most engaging segments to create a trailer.  
ğŸ›  **Technologies**: Python, TensorFlow, Keras, OpenCV, LSTM, CNN

---

### ğŸ“œ **Investigating the Perception of a Nearby Wall in a Virtual Acoustic Environment with Self-produced Oral Sounds**
ğŸ“‚ **GitHub Repo**: [View Code](https://github.com/daoramey/virtual-wall)  
ğŸ“„ **Project Report**: [Read Here]({{ site.baseurl }}/assets/pdfs/Report.pdf)  
ğŸ“„ **Project Presentation**: [Read Here]({{ site.baseurl }}/assets/pdfs/Slides.key)
ğŸš€ **Description**: Developed a spatial audio program using pybinsim to generate a virtual wall effect based on sound reflections and distance perception. The system simulates real-world acoustics to enhance immersion.  
ğŸ›  **Technologies**: Python, pybinsim, Unity, Spatial Audio Processing  

---

### ğŸ“² **Mobile-based AR First-Person Shooting Gamme**
ğŸ“‚ **GitHub Repo**: [View Code](https://github.com/daoramey/ar-fps) 
ğŸš€ **Description**: A mobile-based AR first-person shooting game, developed as a team project for the Game Development course. The game integrates real-world maps to create an immersive augmented reality experience.  
ğŸ›  **Technologies**: Unity, C#, ARKit, ARCore, Mapbox SDK, GPS Mapping  
