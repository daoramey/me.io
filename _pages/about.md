---
layout: single
title: About Me
---

Hello, I'm **Chenyao Diao**, a research engineer dedicated to harnessing advanced technologies to capture and interpret nonverbal human behavior. With over 3 years of experience as a research assistant in telepresence system development, I specialize in using deep learning and computer vision to explore the subtle dynamics of non-verbal communication during remote interaction. My work bridges the gap between technology and human interaction‚Äîdesigning studies and prototypes that model nonverbal communication dynamics.

---

## üîé Research & Interaction Analysis

- **Immersive Communication Systems:**  
  I develop telepresence and immersive platforms to enhance human-avatar interactions in social VR/AR and videoconferencing settings.

- **Quality of Experience Assessment:**  
  I design and conduct user studies that compare non-verbal behaviors under varied experimental conditions, shedding light on how system design influences natural communication.

- **Multi-Modal Data Acquisition:**  
  I build multi-camera and sensor-based recording systems to capture real-world interactions, establishing a robust foundation for analyzing both behavioral and physiological activities.

- **Non-Verbal Communication Analysis:**  
  I create prototypes that extract and analyze a wide array of non-verbal cues‚Äîincluding facial expressions, eye gaze, head and body movements, blink rate, speech activity, turn-taking, interpersonal synchrony, and smile mimicry‚Äîto better understand communication dynamics.

---

## üìã Research Areas

- **Behavioral & Physiological Coordination:** Exploring the synchronization of communication cues.
- **Non-Verbal Behavior Analysis:** Examining the subtleties of body language, eye gaze, and facial expressions.
- **User Interaction Quality:** Assessing and enhancing the experience of digital communication.
- **Human-Agent Interaction:** Improving the realism and responsiveness of virtual avatars and agents.

---

## üìù My Skills

- **Programming & Scripting:** Proficient in Python, R, C, and C# for developing research prototypes, data analysis, and system integration.
- **Operating Systems:** Experienced with Linux environments for software development and system administration.
- **Deep Learning & Machine Learning:** Expertise in applying deep learning frameworks (e.g., PyTorch) to model human behavior.
- **Computer Vision & Image Processing:** Experience with OpenCV, OpenFace, OpenPose, and MediaPipe for analyzing facial expressions, eye gaze, and body motion.
- **Audio & Speech Processing:** Familiar with tools such as OpenSMILE, Silero Models, and Whisper for extracting audio cues and speech activity.
- **Annotation & Data Labeling:** Competent in using ELAN for annotating non-verbal cues like smiles, mutual gaze, and conversational dynamics.
